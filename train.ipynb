{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pku_autonomous_driving import io, util, dataset, resnet, centernet, training, graphics, transform, const, geometry\n",
    "\n",
    "import importlib\n",
    "importlib.reload(geometry)\n",
    "importlib.reload(io)\n",
    "importlib.reload(util)\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(resnet)\n",
    "importlib.reload(centernet)\n",
    "importlib.reload(training)\n",
    "importlib.reload(graphics)\n",
    "importlib.reload(transform)\n",
    "importlib.reload(const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from pku_autonomous_driving.transform import HorizontalFlip, CropBottomHalf, CropFar, PadByMean, Resize, Normalize, DropPointsAtOutOfScreen, CreateMaskAndRegr, ToCHWOrder\n",
    "from pku_autonomous_driving.const import IMG_WIDTH, IMG_HEIGHT, MODEL_SCALE\n",
    "\n",
    "near_transform_funcs = [\n",
    "    HorizontalFlip(),\n",
    "    CropBottomHalf(),\n",
    "    Resize(IMG_WIDTH, IMG_HEIGHT),\n",
    "    Normalize(),\n",
    "    DropPointsAtOutOfScreen(IMG_WIDTH, IMG_HEIGHT),\n",
    "    CreateMaskAndRegr(IMG_WIDTH, IMG_HEIGHT, MODEL_SCALE),\n",
    "    ToCHWOrder()  \n",
    "]\n",
    "\n",
    "near_transform = torchvision.transforms.Compose(near_transform_funcs)\n",
    "near_transform_flip = torchvision.transforms.Compose([\n",
    "    HorizontalFlip(),\n",
    "    near_transform_funcs\n",
    "])\n",
    "\n",
    "far_transform_funcs = [\n",
    "    CropFar(IMG_WIDTH, IMG_HEIGHT),\n",
    "    Normalize(),\n",
    "    DropPointsAtOutOfScreen(IMG_WIDTH, IMG_HEIGHT),\n",
    "    CreateMaskAndRegr(IMG_WIDTH, IMG_HEIGHT, MODEL_SCALE),\n",
    "    ToCHWOrder()\n",
    "]\n",
    "\n",
    "far_transform = torchvision.transforms.Compose(far_transform_funcs)\n",
    "far_transform_flip = torchvision.transforms.Compose([\n",
    "    HorizontalFlip(),\n",
    "    far_transform_funcs\n",
    "])\n",
    "\n",
    "transforms = {\n",
    "    'NEAR': (near_transform, near_transform_flip),\n",
    "    'FAR': (far_transform, far_transform_flip)\n",
    "}\n",
    "\n",
    "train_transform, train_transform_flip = transforms[os.environ.get(\"TRANSFORM_TYPE\", \"NEAR\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pku_autonomous_driving.dataset import CarDataset, create_data_loader\n",
    "from pku_autonomous_driving.const import BATCH_SIZE\n",
    "\n",
    "train, dev = io.load_train_data()\n",
    "\n",
    "use_flip_data = True\n",
    "\n",
    "train_datasets = [CarDataset(train, transform=train_transform)]\n",
    "if use_flip_data:\n",
    "    train_datasets.append(CarDataset(train, transform=train_transform_flip))\n",
    "train_loader = create_data_loader(train_datasets, batch_size=BATCH_SIZE)\n",
    "\n",
    "dev_dataset = CarDataset(dev, transform=train_transform)\n",
    "dev_loader = create_data_loader(dev_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data = train_loader.dataset[0]\n",
    "img, mask, regr = data[\"img\"], data[\"mask\"], data[\"regr\"]\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(np.rollaxis(img, 0, 3))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(mask)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(regr[-2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "base_model = resnet.resnext50_32x4d(pretrained=False)\n",
    "model = centernet.CentResnet(base_model, 7)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "#optimizer =  RAdam(model.parameters(), lr = 0.001)\n",
    "\n",
    "setup_kwargs = {\n",
    "    \"model\": model,\n",
    "    \"device\": device,\n",
    "#    \"path\" : Path(\"./res_mask_1/resnext50.pth\")\n",
    "}\n",
    "\n",
    "util.setup_model(**setup_kwargs)\n",
    "#model = model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = int(os.environ.get(\"N_EPOCHS\", 6))\n",
    "try:\n",
    "    history = pickle.load(Path(os.environ[\"INITIAL_HISTORY\"]).open('rb'))\n",
    "    beg_epoch = math.ceil(history.index[-1])\n",
    "except:\n",
    "    history = pd.DataFrame()\n",
    "    beg_epoch = 0\n",
    "end_epoch = beg_epoch + n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from torch.optim import lr_scheduler\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=max(n_epochs, 10) * len(train_loader) // 3, gamma=0.1)\n",
    "\n",
    "best_dev_loss = np.inf\n",
    "for epoch in range(beg_epoch, end_epoch):\n",
    "    training.clean_up()\n",
    "    training.train(model, optimizer, exp_lr_scheduler, train_loader, epoch, device, history)\n",
    "    training.evaluate(model, dev_loader, epoch, device, history)\n",
    "    training.save_checkpoint(model, optimizer, history)\n",
    "\n",
    "    cur_dev_loss = history['dev_loss'].dropna().iloc[-1]\n",
    "    if cur_dev_loss < best_dev_loss:\n",
    "        torch.save({\"model\": model.state_dict()}, './resnext50.pth')\n",
    "        best_dev_loss = cur_dev_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history['train_loss'].iloc[:].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series1 = history.dropna()['mask_loss']\n",
    "plt.plot(series1.index, series1 ,label = 'mask loss');\n",
    "series2 = history.dropna()['regr_loss']\n",
    "plt.plot(series2.index, 30*series2,label = 'regr loss');\n",
    "series3 = history.dropna()['dev_loss']\n",
    "plt.plot(series3.index, series3,label = 'dev loss');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = history.dropna()['dev_loss']\n",
    "plt.scatter(series.index, series);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_loader.dataset[0]\n",
    "img, mask, regr = data[\"img\"], data[\"mask\"], data[\"regr\"]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.title('Input image')\n",
    "plt.imshow(np.rollaxis(img, 0, 3))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.title('Ground truth mask')\n",
    "plt.imshow(mask)\n",
    "plt.show()\n",
    "\n",
    "output = model(torch.tensor(img[None]).to(device))\n",
    "logits = output[0,0].data.cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.title('Model predictions')\n",
    "plt.imshow(logits)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.title('Model predictions thresholded')\n",
    "plt.imshow(logits > 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "training.clean_up()\n",
    "for idx in range(8):\n",
    "    data = dev_loader.dataset[idx]\n",
    "    img, mask, regr = data[\"img\"], data[\"mask\"], data[\"regr\"]\n",
    "    #output = model(torch.tensor(img[None]).to(device)).data.cpu().numpy()\n",
    "\n",
    "    #coords_pred = util.extract_coords(data, output[0])\n",
    "    coords_true = util.extract_coords(data)\n",
    "\n",
    "    img = io.load_image(dev_loader.dataset.dataset[idx].image_id)\n",
    "    img = img[:,::-1]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(30,30))\n",
    "    axes[0].set_title('Ground truth')\n",
    "    axes[0].imshow(graphics.draw_coords(img, coords_true))\n",
    "    #axes[1].set_title('Prediction')\n",
    "    #axes[1].imshow(graphics.draw_coords(img, coords_pred))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
